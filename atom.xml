<?xml version="1.0"?>
<feed xmlns="http://www.w3.org/2005/Atom">
    <id>http://example.com</id>
    <title>自是白衣卿相的博客</title>
    <subtitle></subtitle>
    <icon>http://example.com/images/favicon.ico</icon>
    <link href="http://example.com" />
    <author>
      <name>Miralce</name>
    </author>
    <updated>2024-02-23T16:00:00.000Z</updated>
    <category term="博客 笔记" />
    <entry>
        <id>http://example.com/2024/02/24/neural_network/transformer/</id>
        <title>Transformer 备忘录</title>
        <link rel="alternate" href="http://example.com/2024/02/24/neural_network/transformer/"/>
        <content type="html">&lt;ol&gt;
&lt;li&gt;
&lt;p&gt;为什么选择 Transformer&lt;br&gt;
NLP 任务需要编码器抽取上下文的特征&lt;br&gt;
上下文的语义很重要，每个词的语义和上下文强相关&lt;br&gt;
方向&lt;br&gt;
 RNN 只能对句子进行单向的编码&lt;br&gt;
 CNN 只能对短句子进行编码&lt;br&gt;
 Transformer 可以双向编码，也抽取长距离的特征&lt;br&gt;
顺序（词的位置变换了会产生不同的意思）&lt;br&gt;
速度&lt;br&gt;
 RNN 无法并行处理序列，其他都行&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;多头自注意力模块&lt;br&gt;
首先对输入进行 projection，得到 QKV 三个矩阵&lt;br&gt;
（tf 对输入进行了降维 (B, L, D) -&amp;gt; (B&lt;em&gt;L, D)）&lt;br&gt;
初始化三个 dense 层 (B&lt;/em&gt;L, D) -&amp;gt; (B&lt;em&gt;L, N&lt;/em&gt;H) N 为 attention 模块的数量，H 为长度&lt;br&gt;
为什么要在这个进行投影呢？&lt;br&gt;
若不投影，在之后的注意力模块中，相同的 Q 和 V 会进行点积，使得 attention 矩阵对角线的分数特别高（自己和自己的分数？），每个词的注意力都在自己身上，无法获得上下文的关系。所以需要将 QKV 投影到不同的空间中，增加多样性。&lt;/p&gt;
&lt;p&gt;之后进行 reshape, (B&lt;em&gt;L, N&lt;/em&gt;H) -&amp;gt; (B, N, L, H)&lt;br&gt;
 为什么要用多注意力头呢？&lt;br&gt;
希望不同的注意力头学到不同的特征，和 CNN 里的 multi-channel 类似&lt;/p&gt;
&lt;p&gt;多不同的注意力头进行点积、转置&lt;br&gt;
为什么要用乘法呢？（为什么不用加性 attention 呢？）&lt;br&gt;
在 GPU 的场景下，矩阵乘法的效率更高，随着 D 的增大，加性 attention 的效果会更好&lt;br&gt;
为什么要除以√D 呢？&lt;br&gt;
两个的矩阵相乘之后方差会变大，使得有些词注意力很大有些很小，经过 softmax 之后再求导会导致梯度很小，不利于优化，除了之后可以平滑一下。e.g. N (0, √d)*N (0, √d) = N (0, d) -&amp;gt; / √d -&amp;gt; N (0, 1)。&lt;/p&gt;
&lt;p&gt;乘 mask 矩阵（正常 token 的值为 0，要丢弃的 token 值为 1w）使得正常 token 的值维持原状，被丢弃的 token 值很小。使得进过 softmax 之后，不想要被注意的值无限趋于 0&lt;/p&gt;
&lt;p&gt;进行 softmax 归一化得到注意力分数&lt;/p&gt;
&lt;p&gt;用 V 矩阵和注意力分数进行加权，再把不同头的分数合并起来&lt;/p&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;残差连接 &amp;amp; Normalisation&lt;br&gt;
 相加操作与 ResNet 相似，相当于在求导时加了一个恒等项，减少梯度消失的问题&lt;/p&gt;
&lt;p&gt;Layer_Norm&lt;br&gt;
 提升神经网络的泛化性&lt;br&gt;
（数据的分布对模型可能会有很大的影响）&lt;br&gt;
所以需要将隐藏层的输出都归一化成均值为 0 方差为 1 的分布&lt;br&gt;
更好的利用模型在训练集中收获的知识&lt;br&gt;
 norm 放在激活函数之前，避免数据落入饱和区，减少梯度消失的问题&lt;br&gt;
（实际操作时会初始化一个新的均值和方差，调整分布，增加多样性）&lt;/p&gt;
&lt;pre&gt;&lt;code&gt; NLP的序列是变长的，Batch_Norm在对不同样本的同一个位置去做归一化时，无法获得真实分布的统计值。
 Layer_Norm会对同一个样本每一个位置的不同特征做归一化


 

 Normalisation可以放在不同的地方
     Post-LayerNorm先做残差再归一化
         保持主干网络的方差比较稳定，使模型泛化能力更强。
         但把恒等的路径放在norm里，会导致模型更难收敛。
     Pre-LayerNorm先归一化再做残差
         更容易收敛，但只是增加了网络的宽度而不是深度，效果没有前者好。
&lt;/code&gt;&lt;/pre&gt;
&lt;/li&gt;
&lt;li&gt;
&lt;p&gt;Positionwise FFN (Feed Forward Network)&lt;br&gt;
 提供了非线性变换，提升拟合能力 (1*1 卷积核的全连接层)。&lt;br&gt;
激活层从 ReLU 变成了 GeLU，引入了正则的思想，越小的值越可能被丢弃掉，相当于 ReLU 和 dropout 的结合。&lt;br&gt;
tanh 和 sigmoid 的双边区域会饱和，导致导数趋于 0，有梯度消失的问题。&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
</content>
        <category term="神经网络" scheme="http://example.com/categories/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" />
        <category term="自然语言处理" scheme="http://example.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" />
        <updated>2024-02-23T16:00:00.000Z</updated>
    </entry>
    <entry>
        <id>http://example.com/2024/02/24/projects/alzheimer/</id>
        <title>Alzheimer’s Disease Detection using Disfluencies Implemented Fine-Tuning BERT Model Ensemble</title>
        <link rel="alternate" href="http://example.com/2024/02/24/projects/alzheimer/"/>
        <content type="html">&lt;ol&gt;
&lt;li&gt;背景介绍&lt;br&gt;
使用原始的、未注释的、未转录的语音进行目标认知状态预测，并解决认知随时间变化的预测。&lt;/li&gt;
&lt;/ol&gt;
&lt;p&gt;address 挑战：阿尔茨海默氏痴呆症识别仅通过自发语言&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;从一个简短的演讲中检测阿尔茨海默氏痴呆症&lt;/li&gt;
&lt;li&gt;认知测试分数的推断，MMSE 分数。&lt;/li&gt;
&lt;li&gt;预测认知能力下降&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;任务:&lt;br&gt;
 从一个简短的演讲中检测阿尔茨海默氏痴呆症&lt;/p&gt;
&lt;p&gt;可能的一般方法:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;直接使用语音信号 (声学特征)&lt;/li&gt;
&lt;li&gt;语音自动转换为文本 (ASR)，并从脚本中提取语言特征&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;更好的解决方案:&lt;br&gt;
 结合声学和语言特征，利用预训练 BERT 模型。&lt;/p&gt;
</content>
        <category term="科研项目" scheme="http://example.com/categories/projects/" />
        <category term="自然语言处理" scheme="http://example.com/tags/%E8%87%AA%E7%84%B6%E8%AF%AD%E8%A8%80%E5%A4%84%E7%90%86/" />
        <updated>2024-02-23T16:00:00.000Z</updated>
    </entry>
    <entry>
        <id>http://example.com/2024/02/18/hello-world/</id>
        <title>Hello World</title>
        <link rel="alternate" href="http://example.com/2024/02/18/hello-world/"/>
        <content type="html">&lt;p&gt;Welcome to &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvLw==&#34;&gt;Hexo&lt;/span&gt;! This is your very first post. Check &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvL2RvY3Mv&#34;&gt;documentation&lt;/span&gt; for more info. If you get any problems when using Hexo, you can find the answer in &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvL2RvY3MvdHJvdWJsZXNob290aW5nLmh0bWw=&#34;&gt;troubleshooting&lt;/span&gt; or you can ask me on &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9naXRodWIuY29tL2hleG9qcy9oZXhvL2lzc3Vlcw==&#34;&gt;GitHub&lt;/span&gt;.&lt;/p&gt;
&lt;h2 id=&#34;quick-start&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#quick-start&#34;&gt;#&lt;/a&gt; Quick Start&lt;/h2&gt;
&lt;h3 id=&#34;create-a-new-post&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#create-a-new-post&#34;&gt;#&lt;/a&gt; Create a new post&lt;/h3&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo new &lt;span class=&#34;string&#34;&gt;&amp;quot;My New Post&amp;quot;&lt;/span&gt;&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvd3JpdGluZy5odG1s&#34;&gt;Writing&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;run-server&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#run-server&#34;&gt;#&lt;/a&gt; Run server&lt;/h3&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo server&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvc2VydmVyLmh0bWw=&#34;&gt;Server&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;generate-static-files&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#generate-static-files&#34;&gt;#&lt;/a&gt; Generate static files&lt;/h3&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo generate&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvL2RvY3MvZ2VuZXJhdGluZy5odG1s&#34;&gt;Generating&lt;/span&gt;&lt;/p&gt;
&lt;h3 id=&#34;deploy-to-remote-sites&#34;&gt;&lt;a class=&#34;anchor&#34; href=&#34;#deploy-to-remote-sites&#34;&gt;#&lt;/a&gt; Deploy to remote sites&lt;/h3&gt;
&lt;figure class=&#34;highlight bash&#34;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&#34;gutter&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;1&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;td class=&#34;code&#34;&gt;&lt;pre&gt;&lt;span class=&#34;line&#34;&gt;$ hexo deploy&lt;/span&gt;&lt;br&gt;&lt;/pre&gt;&lt;/td&gt;&lt;/tr&gt;&lt;/table&gt;&lt;/figure&gt;
&lt;p&gt;More info: &lt;span class=&#34;exturl&#34; data-url=&#34;aHR0cHM6Ly9oZXhvLmlvL2RvY3Mvb25lLWNvbW1hbmQtZGVwbG95bWVudC5odG1s&#34;&gt;Deployment&lt;/span&gt;&lt;/p&gt;
</content>
        <updated>2024-02-18T10:04:35.837Z</updated>
    </entry>
</feed>
